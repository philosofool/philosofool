{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac0d365",
   "metadata": {},
   "source": [
    "# Oddball Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c6c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philosofool/repos/philosofool/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Iterator\n",
    "import json\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans, HDBSCAN    # pyright: ignore [reportAttributeAccessIssue]  HDBSCAN not recognized.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import calinski_harabasz_score, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ebccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'archive/CC GENERAL.csv'\n",
    "# see https://www.kaggle.com/datasets/arjunbhasin2013/ccdata?resource=download for dataset description.\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc6f61",
   "metadata": {},
   "source": [
    "## Data Exploration and Discussion\n",
    "\n",
    "The data is very clean. Missing values are only in CREDIT_LIMIT and MINIMUM_PAYMENT, 1 and ~300 (of 8950) respectively. \n",
    "These are easy to explain: a no-limit card and card-holders with no payment history to date.\n",
    "There are some huge outliers.\n",
    "The the variables are not normally distributed; they tend to center to the left and right in histograms. \n",
    "A few have a bathtub shape.\n",
    "\n",
    "CUST_ID : Identification of Credit Card holder (Categorical)\n",
    "BALANCE : Balance amount left in their account to make purchases\n",
    "BALANCE_FREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n",
    "PURCHASES : Amount of purchases made from account\n",
    "ONEOFF_PURCHASES : Maximum purchase amount done in one-go\n",
    "INSTALLMENTS_PURCHASES : Amount of purchase done in installment\n",
    "CASH_ADVANCE : Cash in advance given by the user\n",
    "PURCHASES_FREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n",
    "ONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n",
    "PURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n",
    "CASHADVANCEFREQUENCY : How frequently the cash in advance being paid\n",
    "CASHADVANCETRX : Number of Transactions made with \"Cash in Advanced\"\n",
    "PURCHASES_TRX : Numbe of purchase transactions made\n",
    "CREDIT_LIMIT : Limit of Credit Card for user\n",
    "PAYMENTS : Amount of Payment done by user\n",
    "MINIMUM_PAYMENTS : Minimum amount of payments made by user\n",
    "PRCFULLPAYMENT : Percent of full payment paid by user\n",
    "TENURE : Tenure of credit card service for user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98458387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Value Counts\n",
      "{'CREDIT_LIMIT': 1, 'MINIMUM_PAYMENTS': 313}\n",
      "Duplicate Record Count\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print('Missing Value Counts')\n",
    "pprint({key: value for key, value in df.isna().sum().items() if value > 0})\n",
    "print(\"Duplicate Record Count\")\n",
    "print(len(df) - len(df.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc56e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(20, 15))\n",
    "# based on below histogram, the following fell like potential differentiators.\n",
    "# We will try clustering on these features, in addition to the full feature set:\n",
    "selected_columns = [column.upper() for column in [\n",
    "    'balance', 'balance_frequency', 'cash_advance_frequency', 'purchases_frequency', 'oneoff_purchases_frequency',\n",
    "    'purchases_installments_frequency', 'cash_advance_frequency', 'credit_limit', 'prc_full_payment'\n",
    "]]\n",
    "assert all(col in df.columns for col in selected_columns), 'You probably have a typo in a name.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c1baa",
   "metadata": {},
   "source": [
    "## Evaluation Process\n",
    "\n",
    "Per my preference, I define evaluation before modeling. This is because:\n",
    "- It's a bad idea to create a model you don't know how to evaluate. It can result in deciding a model has virtues you can find rather than virtues you expect.\n",
    "- It's a good idea to have conversation about how a model will be assess before you can talk about the strengths and weakness of a model. (Teams take responsibility for what counts as good, expressing requirements, etc.)\n",
    "- Is intelletually honest about the quality of evaluation you can produce. \n",
    "(Clustering, for example, does not have an obvious right answer, but depends on application.)\n",
    "- It encourages writing flexible evalution functions, which makes model development and iteration faster in the long run. (Promotes code re-use.)\n",
    "\n",
    "We will also rely on visualization as an intuitive guide to the clusters produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3b6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_labels': 3, 'noise_percent': np.float64(0.0), 'silhouette_score': 0.7172844195939352, 'calinski_harabasz_score': 2303.145483442002}\n"
     ]
    }
   ],
   "source": [
    "def simple_logs(new_data: dict | list | str, path: str):\n",
    "    \"\"\"Append new data to an existing json, or create one if it does not exist.\"\"\"\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "    except FileNotFoundError:\n",
    "        data = []\n",
    "    data.append(new_data)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(json.dumps(data, indent=4))\n",
    "\n",
    "def evaluate_unsupervised_clustering(model: BaseEstimator, data: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate an unsupervised clustering model using common clustering metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : BaseEstimator\n",
    "        A fitted or unfitted scikit-learn compatible clustering model (e.g., KMeans, HDBSCAN).\n",
    "        The model must implement a `.fit_predict` method.\n",
    "\n",
    "    data : np.ndarray\n",
    "        The input data array of shape (n_samples, n_features) to cluster.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scores : dict\n",
    "        A dictionary containing the following metrics:\n",
    "        - 'n_labels': int\n",
    "            The number of non-noise clusters found (ignores label -1).\n",
    "        - 'noise_percent': float\n",
    "            Proportion of data points labeled as noise (-1).\n",
    "        - 'silhouette_score': float\n",
    "            Silhouette score of the clustering (0 if fewer than 2 clusters).\n",
    "        - 'calinski_harabasz_score': float\n",
    "            Calinski-Harabasz index of the clustering (0 if fewer than 2 clusters).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If the model finds fewer than 2 clusters (excluding noise), the silhouette and\n",
    "    Calinski-Harabasz scores will be set to 0, as these metrics are undefined in that case.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    clustered_data = model.fit_predict(data)\n",
    "    labels = np.unique(clustered_data)\n",
    "    scores['n_labels'] = labels.size - (-1 in labels)\n",
    "    scores['noise_percent'] = np.sum((clustered_data == -1)) / clustered_data.size\n",
    "    if scores['n_labels'] > 1:\n",
    "        scores['silhouette_score'] = silhouette_score(data, clustered_data, metric='euclidean')\n",
    "        scores['calinski_harabasz_score'] = calinski_harabasz_score(data, clustered_data)\n",
    "    else:\n",
    "        scores['silhouette_score'] = 0\n",
    "        scores['calinski_harabasz_score'] = 0\n",
    "    return scores\n",
    "\n",
    "\n",
    "def test_evaluate_unsupervised_clustering():\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    model = KMeans(n_clusters=3, random_state=123)\n",
    "    data, *_ = make_blobs(random_state=567)\n",
    "\n",
    "    result = evaluate_unsupervised_clustering(model, data)\n",
    "    assert result['silhouette_score'] > 0\n",
    "    assert result['calinski_harabasz_score'] > 0\n",
    "    assert result['noise_percent'] >= 0 and result['noise_percent'] <= 1.\n",
    "    assert result['n_labels'] == 3\n",
    "\n",
    "\n",
    "test_evaluate_unsupervised_clustering()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd8f84",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "We will rely somewhat on vissualization to guide our sense of model quality.\n",
    "This relies on subjective intuitions.\n",
    "\n",
    "DISCLAIMER: The visualization code here is from https://scikit-learn.org/stable/auto_examples/cluster/plot_hdbscan.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(X, labels, probabilities=None, parameters=None, ground_truth=False, ax=None):\n",
    "    \"\"\"\n",
    "    Visualize a clustering model in 2D, optionally showing cluster probabilities and parameters.\n",
    "\n",
    "    SOURCE by: https://scikit-learn.org/stable/auto_examples/cluster/plot_hdbscan.html\n",
    "    DOCUMENTATION: ChatGPT\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray of shape (n_samples, 2)\n",
    "        The 2D data points to plot, typically obtained by dimensionality reduction (e.g. PCA, UMAP).\n",
    "\n",
    "    labels : array-like of shape (n_samples,)\n",
    "        Cluster labels for each point. Noise points should be labeled `-1`.\n",
    "\n",
    "    probabilities : array-like of shape (n_samples,), optional\n",
    "        Optional array of cluster membership probabilities for each point. Used to scale marker size.\n",
    "        If None, all probabilities are assumed to be 1.\n",
    "\n",
    "    parameters : dict, optional\n",
    "        Optional dictionary of model parameters to include in the plot title.\n",
    "\n",
    "    ground_truth : bool, default=False\n",
    "        Whether the labels represent ground truth. Affects the plot title.\n",
    "\n",
    "    ax : matplotlib.axes.Axes, optional\n",
    "        Axes object to draw the plot on. If None, a new figure and axes are created.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Clustered points are shown in different colors.\n",
    "    - Noise points (label `-1`) are shown in black 'x' markers.\n",
    "    - Marker sizes are scaled by cluster membership probabilities.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(10, 4))\n",
    "    labels = labels if labels is not None else np.ones(X.shape[0])\n",
    "    probabilities = probabilities if probabilities is not None else np.ones(X.shape[0])\n",
    "    # Black removed and is used for noise instead.\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    # The probability of a point belonging to its labeled cluster determines\n",
    "    # the size of its marker\n",
    "    proba_map = {idx: probabilities[idx] for idx in range(len(labels))}\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "\n",
    "        class_index = (labels == k).nonzero()[0]\n",
    "        for ci in class_index:\n",
    "            ax.plot(\n",
    "                X[ci, 0],\n",
    "                X[ci, 1],\n",
    "                \"x\" if k == -1 else \"o\",\n",
    "                markerfacecolor=tuple(col),\n",
    "                markeredgecolor=\"k\",\n",
    "                markersize=4 if k == -1 else 1 + 5 * proba_map[ci],\n",
    "                alpha=.8\n",
    "            )\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    preamble = \"True\" if ground_truth else \"Estimated\"\n",
    "    title = f\"{preamble} number of clusters: {n_clusters_}\"\n",
    "    if parameters is not None:\n",
    "        parameters_str = \", \".join(f\"{k}={v}\" for k, v in parameters.items())\n",
    "        title += f\" | {parameters_str}\"\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def visualize_2d_repr(model: Pipeline, params: dict, df: pd.DataFrame):\n",
    "    \"\"\"Visualize a model over the first two principle components of the data.\"\"\"\n",
    "    model.set_params(**params)\n",
    "    clustering = model.fit(clean_data(df)).named_steps['clustering']\n",
    "    scaler = model.named_steps['scale_data']\n",
    "    pca = model.named_steps['PCA']\n",
    "    scaled_data = scaler.transform(clean_data(df))\n",
    "    two_d_representation = pca.transform(scaled_data)[:, :2]\n",
    "    if hasattr(clustering, 'propabilities_'):\n",
    "        plot(two_d_representation, clustering.labels_, clustering.probabilities_)\n",
    "    else:\n",
    "        plot(two_d_representation, clustering.labels_)\n",
    "\n",
    "\n",
    "def sanity_check_plot():\n",
    "    data, *_ = make_blobs(500, 10, centers=4, random_state=123687)\n",
    "    model = HDBSCAN().fit(data)\n",
    "    labels = model.labels_\n",
    "    probabilities = model.probabilities_\n",
    "    plot(data, labels, probabilities)\n",
    "\n",
    "sanity_check_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b8be0",
   "metadata": {},
   "source": [
    "## Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bddef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame, selected_columns: list[str] = []) -> np.ndarray:\n",
    "    if selected_columns:\n",
    "        df = df[selected_columns]\n",
    "    else:\n",
    "        df = df.drop(columns=['CUST_ID'])\n",
    "    data = ColumnTransformer(\n",
    "        [step for step in [\n",
    "            ('min_payment', SimpleImputer(strategy='constant', fill_value=-1.), ['MINIMUM_PAYMENTS']),\n",
    "            ('credit_limit', SimpleImputer(strategy='constant', fill_value=200_000.), ['CREDIT_LIMIT']),\n",
    "        ]\n",
    "        if step[2][0] in selected_columns or not selected_columns],\n",
    "        remainder='passthrough'\n",
    "    ).fit_transform(df)\n",
    "    return data\n",
    "\n",
    "def fit_and_evaluate_model(model: BaseEstimator, data_transform: Pipeline, df: pd.DataFrame = df):\n",
    "    data = clean_data(df)\n",
    "    transformed_data = data_transform.fit_transform(data)\n",
    "    return evaluate_unsupervised_clustering(model, transformed_data)\n",
    "\n",
    "def param_grid_to_parameters(param_grid: dict[str, list]) -> Iterator:\n",
    "    parameters = list(param_grid)\n",
    "    for param_options in itertools.product(*param_grid.values()):\n",
    "        param_spec = {parameters[i]: param_value for i, param_value in enumerate(param_options)}\n",
    "        yield param_spec\n",
    "\n",
    "def tune(model_name: str, pipeline: Pipeline, df: pd.DataFrame, param_grid: dict):\n",
    "    \"\"\"Extract parameters from a parameters grid and trained the re-parameterized model on the data in df.\"\"\"\n",
    "    parameter_grid = param_grid[model_name]\n",
    "    for parameters in param_grid_to_parameters(parameter_grid):\n",
    "        pipeline.set_params(**parameters)\n",
    "        data_transform = pipeline.named_steps['data_transform']\n",
    "        cluster_model = pipeline.named_steps['clustering']\n",
    "        scores = fit_and_evaluate_model(cluster_model, data_transform, df)\n",
    "        scores['model_name'] = model_name\n",
    "        results = {'parameters': parameters, 'scores': scores}\n",
    "        simple_logs(results, 'model_scores.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d517fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data Preprocessing and Model Architecture\n",
    "# This will be passed through a tuning process, so we can mostly use default values now.\n",
    "\n",
    "data_transform_standard_pca = Pipeline([\n",
    "    ('scale_data', StandardScaler()),\n",
    "    ('PCA', PCA(n_components=4)),\n",
    "])\n",
    "data_transform_umap = Pipeline([\n",
    "    ('scale_data', StandardScaler()),\n",
    "    ('UMAP', UMAP(n_components=30)),\n",
    "])\n",
    "data_transform_robust_pca = Pipeline([\n",
    "    ('scale_data', RobustScaler()),\n",
    "    ('PCA', PCA(n_components=3)),\n",
    "])\n",
    "\n",
    "clustering_models: dict[str, Pipeline] = {\n",
    "    'kmeans_model': Pipeline([\n",
    "        ('data_transform', data_transform_standard_pca),\n",
    "        ('clustering', KMeans(n_clusters=4))\n",
    "    ]),\n",
    "    'normalize_kmeans': Pipeline([\n",
    "        ('data_transform', data_transform_standard_pca),\n",
    "        ('clustering', KMeans(n_clusters=4))\n",
    "    ]),\n",
    "    'hdb_model': Pipeline([\n",
    "        ('data_transform', data_transform_standard_pca),\n",
    "        ('clustering', HDBSCAN())\n",
    "    ]),\n",
    "    'robust_hdb_model': Pipeline([\n",
    "        ('data_transform', data_transform_robust_pca),\n",
    "        ('clustering', HDBSCAN(min_cluster_size=50))\n",
    "    ]),\n",
    "    'umap_hdb_model': Pipeline([\n",
    "        ('data_transform', data_transform_umap),\n",
    "        ('clustering', HDBSCAN(min_cluster_size=50))\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d7e4f",
   "metadata": {},
   "source": [
    "### Simple Validation\n",
    "\n",
    "Sanity check that the model will produce three clusters from synthetic data with three clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "\n",
    "hdb_param_grid = {\n",
    "    'data_transform__PCA__n_components': [2, 3, 4, 5, .95],\n",
    "    'clustering__min_cluster_size': [5, 25, 40, 50, 60],\n",
    "    'clustering__min_samples': [None, 3, 7, 20],\n",
    "}\n",
    "kmeans_param_grid = {\n",
    "    'data_transform__PCA__n_components': [2, 3, 4, 5, .95],\n",
    "    'clustering__n_clusters': [2, 3, 4, 5]\n",
    "}\n",
    "umap_param_grid = {\n",
    "    'data_transform__UMAP__n_neighbors': [30],\n",
    "    'data_transform__UMAP__n_components': [2, 3],\n",
    "    'clustering__min_cluster_size': [5, 25, 40],\n",
    "    'clustering__min_samples': [None, 3, 7, 20],\n",
    "}\n",
    "\n",
    "clustering_models_params = {\n",
    "    'kmeans_model': kmeans_param_grid,\n",
    "    'normalize_kmeans': kmeans_param_grid,\n",
    "    'hdb_model': hdb_param_grid,\n",
    "    'robust_hdb_model': hdb_param_grid,\n",
    "    'umap_hdb_model': umap_param_grid\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, pipeline in clustering_models.items():\n",
    "    tune(model_name, pipeline, df, clustering_models_params)\n",
    "\n",
    "\n",
    "def best_models(path: str, k_best: int):\n",
    "    \"\"\"Return the models sorted by silhouette score.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        results = json.loads(f.read())\n",
    "        results = sorted(results, key=lambda x: x['scores']['silhouette_score'], reverse=True)\n",
    "    if k_best is None:\n",
    "        return results\n",
    "    return results[:k_best]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
