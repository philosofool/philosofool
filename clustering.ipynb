{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac0d365",
   "metadata": {},
   "source": [
    "# Oddball Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c6c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Iterator\n",
    "import json\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans, HDBSCAN, DBSCAN    # pyright: ignore [reportAttributeAccessIssue]  HDBSCAN not recognized.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import calinski_harabasz_score, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'archive/CC GENERAL.csv'\n",
    "# see https://www.kaggle.com/datasets/arjunbhasin2013/ccdata?resource=download for dataset description.\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc6f61",
   "metadata": {},
   "source": [
    "## Data Exploration and Discussion\n",
    "\n",
    "The data is very clean. Missing values are only in CREDIT_LIMIT and MINIMUM_PAYMENT, 1 and ~300 (of 8950) respectively. \n",
    "These are easy to explain: a no-limit card and card-holders with no payment history to date.\n",
    "There are some huge outliers.\n",
    "The the variables are not normally distributed; they tend to center to the left and right in histograms. \n",
    "A few have a bathtub shape.\n",
    "\n",
    "CUST_ID : Identification of Credit Card holder (Categorical)\n",
    "\n",
    "BALANCE : Balance amount left in their account to make purchases\n",
    "\n",
    "BALANCE_FREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n",
    "\n",
    "PURCHASES : Amount of purchases made from account\n",
    "\n",
    "ONEOFF_PURCHASES : Maximum purchase amount done in one-go\n",
    "\n",
    "INSTALLMENTS_PURCHASES : Amount of purchase done in installment\n",
    "\n",
    "CASH_ADVANCE : Cash in advance given by the user\n",
    "\n",
    "PURCHASES_FREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n",
    "\n",
    "ONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n",
    "\n",
    "PURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n",
    "\n",
    "CASHADVANCEFREQUENCY : How frequently the cash in advance being paid\n",
    "\n",
    "CASHADVANCETRX : Number of Transactions made with \"Cash in Advanced\"\n",
    "\n",
    "PURCHASES_TRX : Numbe of purchase transactions made\n",
    "\n",
    "CREDIT_LIMIT : Limit of Credit Card for user\n",
    "\n",
    "PAYMENTS : Amount of Payment done by user\n",
    "\n",
    "MINIMUM_PAYMENTS : Minimum amount of payments made by user\n",
    "\n",
    "PRCFULLPAYMENT : Percent of full payment paid by user\n",
    "\n",
    "TENURE : Tenure of credit card service for user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98458387",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing Value Counts')\n",
    "pprint({key: value for key, value in df.isna().sum().items() if value > 0})\n",
    "print(\"Duplicate Record Count\")\n",
    "print(len(df) - len(df.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc56e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c1baa",
   "metadata": {},
   "source": [
    "## Evaluation Process\n",
    "\n",
    "Per my preference, I define evaluation before modeling. This is because:\n",
    "- It's a bad idea to create a model you don't know how to evaluate. It can result in deciding a model has virtues you can find rather than virtues you expect.\n",
    "- It's a good idea to have conversation about how a model will be assess before you can talk about the strengths and weakness of a model. (Teams take responsibility for what counts as good, expressing requirements, etc.)\n",
    "- Is intelletually honest about the quality of evaluation you can produce. \n",
    "(Clustering, for example, does not have an obvious right answer, but depends on application.)\n",
    "- It encourages writing flexible evalution functions, which makes model development and iteration faster in the long run. (Promotes code re-use.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_logs(new_data: dict | list | str, path: str):\n",
    "    \"\"\"Append new data to an existing json, or create one if it does not exist.\"\"\"\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "    except FileNotFoundError:\n",
    "        data = []\n",
    "    data.append(new_data)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(json.dumps(data, indent=4))\n",
    "\n",
    "def evaluate_unsupervised_clustering(model: BaseEstimator, data: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate an unsupervised clustering model using common clustering metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : BaseEstimator\n",
    "        A fitted or unfitted scikit-learn compatible clustering model (e.g., KMeans, HDBSCAN).\n",
    "        The model must implement a `.fit_predict` method.\n",
    "\n",
    "    data : np.ndarray\n",
    "        The input data array of shape (n_samples, n_features) to cluster.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scores : dict\n",
    "        A dictionary containing the following metrics:\n",
    "        - 'n_labels': int\n",
    "            The number of non-noise clusters found (ignores label -1).\n",
    "        - 'noise_percent': float\n",
    "            Proportion of data points labeled as noise (-1).\n",
    "        - 'silhouette_score': float\n",
    "            Silhouette score of the clustering (0 if fewer than 2 clusters).\n",
    "        - 'calinski_harabasz_score': float\n",
    "            Calinski-Harabasz index of the clustering (0 if fewer than 2 clusters).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If the model finds fewer than 2 clusters (excluding noise), the silhouette and\n",
    "    Calinski-Harabasz scores will be set to 0, as these metrics are undefined in that case.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    clustered_data = model.fit_predict(data)\n",
    "    labels = np.unique(clustered_data)\n",
    "    value_counts = pd.Series(clustered_data).value_counts()\n",
    "    if value_counts.get(-1):\n",
    "        del value_counts[-1]\n",
    "    scores['n_labels'] = labels.size - (-1 in labels)\n",
    "    scores['noise_percent'] = np.sum((clustered_data == -1)) / clustered_data.size\n",
    "    if not value_counts.empty:\n",
    "        scores['largest_cluster_size'] = int(value_counts.max())\n",
    "        scores['smallest_cluster_size'] = int(value_counts.min())\n",
    "    else:\n",
    "        # possible 100% noise.\n",
    "        scores['largest_cluster_size'] = 0\n",
    "        scores['smallest_cluster_size'] = 0\n",
    "    if scores['n_labels'] > 1:\n",
    "        scores['silhouette_score'] = silhouette_score(data, clustered_data, metric='euclidean')\n",
    "        scores['calinski_harabasz_score'] = calinski_harabasz_score(data, clustered_data)\n",
    "    else:\n",
    "        scores['silhouette_score'] = 0\n",
    "        scores['calinski_harabasz_score'] = 0\n",
    "    return scores\n",
    "\n",
    "\n",
    "def test_evaluate_unsupervised_clustering():\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    model = KMeans(n_clusters=3, random_state=123)\n",
    "    data, *_ = make_blobs(random_state=567)\n",
    "\n",
    "    result = evaluate_unsupervised_clustering(model, data)\n",
    "    assert result['silhouette_score'] > 0\n",
    "    assert result['calinski_harabasz_score'] > 0\n",
    "    assert result['noise_percent'] >= 0 and result['noise_percent'] <= 1.\n",
    "    assert result['n_labels'] == 3\n",
    "\n",
    "\n",
    "test_evaluate_unsupervised_clustering()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b8be0",
   "metadata": {},
   "source": [
    "## Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bddef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame, selected_columns: list[str] = []) -> np.ndarray:\n",
    "    if selected_columns:\n",
    "        df = df[selected_columns]\n",
    "    else:\n",
    "        df = df.drop(columns=['CUST_ID'])\n",
    "    data = ColumnTransformer(\n",
    "        [step for step in [\n",
    "            ('min_payment', SimpleImputer(strategy='constant', fill_value=-1.), ['MINIMUM_PAYMENTS']),\n",
    "            ('credit_limit', SimpleImputer(strategy='constant', fill_value=200_000.), ['CREDIT_LIMIT']),\n",
    "        ]\n",
    "        if step[2][0] in selected_columns or not selected_columns],\n",
    "        remainder='passthrough'\n",
    "    ).fit_transform(df)\n",
    "    return data\n",
    "\n",
    "def fit_and_evaluate_model(model: BaseEstimator, data_transform: Pipeline, df: pd.DataFrame = df):\n",
    "    data = clean_data(df)\n",
    "    transformed_data = data_transform.fit_transform(data)\n",
    "    return evaluate_unsupervised_clustering(model, transformed_data)\n",
    "\n",
    "def param_grid_to_parameters(param_grid: dict[str, list]) -> Iterator:\n",
    "    parameters = list(param_grid)\n",
    "    for param_options in itertools.product(*param_grid.values()):\n",
    "        param_spec = {parameters[i]: param_value for i, param_value in enumerate(param_options)}\n",
    "        yield param_spec\n",
    "\n",
    "def tune(model_name: str, pipeline: Pipeline, df: pd.DataFrame, param_grid: dict):\n",
    "    \"\"\"Extract parameters from a parameters grid and trained the re-parameterized model on the data in df.\"\"\"\n",
    "    parameter_grid = param_grid[model_name]\n",
    "    for parameters in param_grid_to_parameters(parameter_grid):\n",
    "        pipeline.set_params(**parameters)\n",
    "        data_transform = pipeline.named_steps['data_transform']\n",
    "        cluster_model = pipeline.named_steps['clustering']\n",
    "        scores = fit_and_evaluate_model(cluster_model, data_transform, df)\n",
    "        scores['model_name'] = model_name\n",
    "        results = {'parameters': parameters, 'scores': scores}\n",
    "        simple_logs(results, 'model_scores.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d517fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data Preprocessing and Model Architecture\n",
    "# This will be passed through a tuning process, so we can mostly use default values now.\n",
    "\n",
    "data_transform_standard_pca = Pipeline([\n",
    "    ('scale_data', StandardScaler()),\n",
    "    ('PCA', PCA(n_components=4)),\n",
    "])\n",
    "data_transform_umap = Pipeline([\n",
    "    ('scale_data', StandardScaler()),\n",
    "    ('UMAP', UMAP(n_components=30)),\n",
    "])\n",
    "data_transform_robust_pca = Pipeline([\n",
    "    ('scale_data', RobustScaler()),\n",
    "    ('PCA', PCA(n_components=3)),\n",
    "])\n",
    "\n",
    "clustering_models: dict[str, Pipeline] = {\n",
    "    'kmeans_model': Pipeline([\n",
    "        ('data_transform', data_transform_standard_pca),\n",
    "        ('clustering', KMeans(n_clusters=4))\n",
    "    ]),\n",
    "    'normalize_kmeans': Pipeline([\n",
    "        ('data_transform', data_transform_standard_pca),\n",
    "        ('clustering', KMeans(n_clusters=4))\n",
    "    ]),\n",
    "    'hdb_model': Pipeline([\n",
    "        ('data_transform', data_transform_standard_pca),\n",
    "        ('clustering', HDBSCAN())\n",
    "    ]),\n",
    "    'robust_hdb_model': Pipeline([\n",
    "        ('data_transform', data_transform_robust_pca),\n",
    "        ('clustering', HDBSCAN(min_cluster_size=50))\n",
    "    ]),\n",
    "    'umap_hdb_model': Pipeline([\n",
    "        ('data_transform', data_transform_umap),\n",
    "        ('clustering', HDBSCAN(min_cluster_size=50))\n",
    "    ]),\n",
    "    'db_model': Pipeline([\n",
    "        ('data_transform', data_transform_standard_pca),\n",
    "        ('clustering', DBSCAN())\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "\n",
    "hdb_param_grid = {\n",
    "    'data_transform__PCA__n_components': [2, 3, 4],\n",
    "    'clustering__min_cluster_size': [100, 500],\n",
    "    'clustering__min_samples': [None, 3, 7, 20],\n",
    "}\n",
    "kmeans_param_grid = {\n",
    "    'data_transform__PCA__n_components': [2, 3, 4, 5, .95],\n",
    "    'clustering__n_clusters': [2, 3, 4, 5]\n",
    "}\n",
    "umap_param_grid = {\n",
    "    # 'data_transform__UMAP__n_neighbors': [3, 5, 10, 30],\n",
    "    'data_transform__UMAP__n_neighbors': [40, 50, 75],\n",
    "    'data_transform__UMAP__n_components': [2],\n",
    "    'clustering__min_cluster_size': [100, 500],\n",
    "    'clustering__min_samples': [None, 3, 7, 20],\n",
    "}\n",
    "db_param_grid = {\n",
    "    'data_transform__PCA__n_components': [2, 3, 4],\n",
    "    'clustering__eps': [.3, .5, .75, 1.],\n",
    "    # 'clustering__min_samples': [None, 3, 7, 20],\n",
    "}\n",
    "\n",
    "clustering_models_params = {\n",
    "    'kmeans_model': kmeans_param_grid,\n",
    "    'normalize_kmeans': kmeans_param_grid,\n",
    "    'hdb_model': hdb_param_grid,\n",
    "    'robust_hdb_model': hdb_param_grid,\n",
    "    'umap_hdb_model': umap_param_grid,\n",
    "    'db_model': db_param_grid\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, pipeline in clustering_models.items():\n",
    "    ...\n",
    "    # UNCOMMENT TO RUN. Takes a ~5 minutes.\n",
    "    # tune(model_name, pipeline, df, clustering_models_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4930d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_models() -> list[dict]:\n",
    "    \"\"\"\n",
    "    Load and filter clustering model results based on quality criteria.\n",
    "\n",
    "    This function reads model evaluation results from a JSON file (`model_scores.json`)\n",
    "    and returns a list of models that meet specific quality thresholds. These thresholds\n",
    "    are used to filter out low-quality or poorly performing clustering models.\n",
    "\n",
    "    The filtering criteria are:\n",
    "    - The number of clusters (`n_labels`) is between 2 and 7 (inclusive).\n",
    "    - The proportion of noise points (`noise_percent`) is less than or equal to 0.25.\n",
    "    - The silhouette score is at least 0.3.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dict\n",
    "        A list of model evaluation dictionaries that pass the quality filter.\n",
    "        Each dictionary contains a 'scores' field and other metadata.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The input file `model_scores.json` must exist in the current working directory\n",
    "    and must contain a JSON array of model evaluation results, each with a 'scores'\n",
    "    dictionary containing the keys:\n",
    "    - 'n_labels'\n",
    "    - 'noise_percent'\n",
    "    - 'silhouette_score'\n",
    "    \"\"\"\n",
    "    with open(\"model_scores.json\", 'r') as f:\n",
    "        tuning_data = json.loads(f.read())\n",
    "\n",
    "    def is_acceptable(score_data: dict) -> bool:\n",
    "        score = score_data['scores']\n",
    "        return (\n",
    "            score['n_labels'] >= 2\n",
    "            and score['largest_cluster_size'] < 7000\n",
    "            and score['n_labels'] <= 7\n",
    "            and score['noise_percent'] <= .25\n",
    "            and score['silhouette_score'] >= .3\n",
    "        )\n",
    "    return [score_data for score_data in tuning_data if is_acceptable(score_data)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5cbe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results to a dataframe. List best models with more than 2 labels.\n",
    "acceptable_models = quality_models()\n",
    "scores = pd.DataFrame.from_records([a['scores'] | a['parameters'] for a in acceptable_models])\n",
    "scores.sort_values('silhouette_score', ascending=False).query(\"n_labels > 2\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort_values('silhouette_score', ascending=False).query(\"n_labels == 2\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc202bab",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "\n",
    "Our decision criteria were:\n",
    "- producing large enough groups to be potentially useful.\n",
    "- producing few enough groups to be potentially useful.\n",
    "- the silhouette score of the model. (calinski_harabasz_score is well correlated.)\n",
    "- low noise (for relevant models.)\n",
    "\n",
    "### KMeans\n",
    "\n",
    "DBSCAN and HDBSCAN approaches produced no useful models. \n",
    "Either the clusters were too large, the noise too high, or the score too low.\n",
    "\n",
    "KMeans models with 3 labels performed best. Two (Models 1 and 14), based on 2 principle components, scored .45 silhouette.\n",
    "Both had a single very large group, around 6050 members and a modest small group, around 1200 members.\n",
    "KMeans classifies all samples (no noise.)\n",
    "Selection between these is arbitrary.\n",
    "\n",
    "One 4 label model, model 2, is also of interest. It has a lower score, .41, but it's largest group is 3907 members,\n",
    "suggesting an interesting additional partition.\n",
    "\n",
    "\n",
    "### Process Assessment\n",
    "\n",
    "We tuned a large range of hyperparameters to find the above models.\n",
    "It is noteworthy that seemingly adjacent models in the parameter space sometimes diverged in scores.\n",
    "These models fit to this data quickly and there is little concern that more exploration would be too computationally expensive.\n",
    "Along these lines, investigation of DBSCAN and other clustering models is also recommended. \n",
    "I looked briefly into feature engineering and feature removal. \n",
    "This produced no obvious added benefit, but further investigation is needed to determine the value of features selection and engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62a44f",
   "metadata": {},
   "source": [
    "## Cluster Characterisitcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model(scores: pd.DataFrame, index: int) -> Pipeline:\n",
    "    \"\"\"Get a model by index from scores data.\"\"\"\n",
    "    model_spec = scores.loc[index]\n",
    "    model_name = model_spec['model_name']\n",
    "    parameters = list(clustering_models_params[model_name])\n",
    "    param_values = model_spec[parameters]\n",
    "    for key, value in param_values.items():\n",
    "        if key == 'data_transform__PCA__n_components':\n",
    "            if value < 1:\n",
    "                # we're using a quantile to select the number of PCA components.\n",
    "                param_values[key] = value\n",
    "                continue\n",
    "            param_values[key] = int(value)\n",
    "        elif np.isnan(value):\n",
    "            # this indicates we should use the default, but `nan` won't be accepted as the default value.\n",
    "            param_values[key] = None\n",
    "            # del(param_values[key])\n",
    "            # continue\n",
    "        else:\n",
    "            param_values[key] = int(value)\n",
    "    print(param_values)\n",
    "    model = clustering_models[model_name]\n",
    "    model.set_params(**param_values.to_dict())\n",
    "    return model\n",
    "\n",
    "\n",
    "def cluster_characteristics(model: Pipeline, df: pd.DataFrame):\n",
    "    \"\"\"Visualize Cluster Characteristics.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['cluster'] = model.named_steps['clustering'].labels_\n",
    "\n",
    "    selected_features = ['BALANCE_FREQUENCY', 'PAYMENTS', 'CREDIT_LIMIT', 'PURCHASES', 'BALANCE']\n",
    "    assert all(feature in df for feature in selected_features), \"You probably have a typo.\"\n",
    "\n",
    "    feature_combinations = list(itertools.combinations(selected_features, 2))\n",
    "    n_cols = 3\n",
    "    n_rows = (len(feature_combinations)) // n_cols + (1 if len(feature_combinations) % n_cols else 0)\n",
    "    fig, ax = plt.subplots(n_rows, n_cols)\n",
    "    fig.set_size_inches(4 * n_rows, 6 * n_cols)\n",
    "    for i, (x_feature, y_feature) in enumerate(feature_combinations):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        current_ax = ax[row][col]\n",
    "        x = df[x_feature]\n",
    "        y = df[y_feature]\n",
    "        current_ax.scatter(x, y, c=df['cluster'], alpha=.5)\n",
    "        current_ax.set_xlabel(x_feature)\n",
    "        current_ax.set_ylabel(y_feature)\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49303ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_14 = extract_model(scores, 14)\n",
    "model_14.fit(clean_data(df))\n",
    "print(pd.Series(model_14.named_steps['clustering'].labels_).value_counts())\n",
    "cluster_characteristics(model_14, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d40685",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = extract_model(scores, 2)\n",
    "model_2.fit(clean_data(df))\n",
    "print(pd.Series(model_2.named_steps['clustering'].labels_).value_counts())\n",
    "cluster_characteristics(model_2, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b80332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
