{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768dc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "import hashlib\n",
    "import inspect\n",
    "import itertools\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToTensor, Compose, Resize\n",
    "\n",
    "from philosofool.torch.nn_models import (\n",
    "    ResidualBlock, ResidualNetwork, NeuralNetwork,\n",
    "    Generator, Discriminator,\n",
    "    compute_convolution_dims, conv_dims_1d\n",
    ")\n",
    "from philosofool.torch.nn_loop import (\n",
    "    TrainingLoop, JSONLogger, CompositeLogger, StandardOutputLogger, GANLoop, TrainingLoop,\n",
    "    EndOnBatchCallback, SnapshotCallback, VerboseTrainingCallback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img):\n",
    "    \"\"\"Show image implied by input tensor. The input is assumed to be normalized.\"\"\"\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.detach().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92195e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(path) -> str:\n",
    "    with open(path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def get_fashionMINST_data() -> tuple[DataLoader, DataLoader, dict]:\n",
    "\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root='data', train=True, download=True, transform=ToTensor()\n",
    "    )\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root='data', train=False, download=True, transform=ToTensor()\n",
    "    )\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    classes = {\n",
    "        0: 'T-shirt/top',\n",
    "        1: 'Trouser',\n",
    "        2: 'Pullover',\n",
    "        3: 'Dress',\n",
    "        4: 'Coat',\n",
    "        5: 'Sandal',\n",
    "        6: 'Shirt',\n",
    "        7: 'Sneaker',\n",
    "        8: 'Bag',\n",
    "        9: 'Ankle'}\n",
    "    return train_dataloader, test_dataloader, classes\n",
    "\n",
    "def get_food100_data():\n",
    "    transform = Compose([Resize((256, 256)), ToTensor()])\n",
    "    training_data = datasets.Food101(\n",
    "        root='data', split='train', download=True, transform=transform\n",
    "    )\n",
    "    test_data = datasets.Food101(\n",
    "        root='data', split='test', download=True, transform=transform\n",
    "    )\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(\n",
    "        training_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "    classes_path = os.path.join(os.getcwd(), 'data/food-101/meta/classes.txt')\n",
    "    classes = dict(enumerate(read_text(classes_path).split('\\n')))\n",
    "\n",
    "    return train_dataloader, test_dataloader, classes\n",
    "\n",
    "\n",
    "train_dataloader, test_dataloader, classes = get_fashionMINST_data()\n",
    "# train_dataloader, test_dataloader, classes = get_food100_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c700a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "input_dims = tuple(int(x) for x in X.shape[2:])\n",
    "in_channels = int(X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "161c09b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37dc1549",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "configurations = [\n",
    "    ('baseline', {'n_hidden_layers': 1, 'width': 256}),\n",
    "    ('h2_w128', {'n_hidden_layers': 2, 'width': 128}),\n",
    "    ('h3_w94', {'n_hidden_layers': 3, 'width': 96}),\n",
    "    ('h4_w64', {'n_hidden_layers': 4, 'width': 64})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "593026cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_experiment(configurations):\n",
    "    X, _ = next(iter(test_dataloader))\n",
    "    input_dims = int(X.shape[2] * X.shape[3])\n",
    "    input_channels = int(X.shape[1])\n",
    "    for model_name, config in configurations:\n",
    "        model = NeuralNetwork(input_dims * input_channels, len(classes), **config)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        logger = CompositeLogger(JSONLogger('data/logs/food100_' + model_name + '.json'), StandardOutputLogger(500))\n",
    "        training_loop = TrainingLoop(model, optimizer, loss_fn, logger)\n",
    "        print(model_name)\n",
    "        training_loop.fit(train_dataloader, test_dataloader, epochs=1)\n",
    "\n",
    "# model_experiment(configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a97429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_model_path(model: nn.Module, model_name: str, directory: str | None) -> str:\n",
    "    string_hash = hashlib.sha1(bytes(str(model).encode())).hexdigest()\n",
    "    full_path = model_name + '_' + string_hash+ '.model'\n",
    "    if directory:\n",
    "        full_path = os.path.join(directory, full_path)\n",
    "    return os.path.normpath(full_path)\n",
    "\n",
    "\n",
    "def save_model(model: nn.Module, model_name: str, directory: str | None):\n",
    "    \"\"\"Save model, using string repr to assure that models are not duplicated once trained.\"\"\"\n",
    "    full_path = _make_model_path(model, model_name, directory)\n",
    "    if not os.path.exists(os.path.dirname(full_path)):\n",
    "        os.makedirs(os.path.dirname(full_path))\n",
    "    torch.save(model, full_path)\n",
    "\n",
    "def load_model(model: nn.Module, model_name: str, directory: str | None):\n",
    "    full_path = _make_model_path(model, model_name, directory)\n",
    "    torch.load(full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e05363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_model = ResidualNetwork((256, 256), in_channels, 101).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c6404de",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(resid_model.parameters(), lr=1e-3)\n",
    "logger = CompositeLogger(JSONLogger('food100_conv_baseline.json'), StandardOutputLogger(100))\n",
    "\n",
    "training_loop = TrainingLoop(resid_model, optimizer, loss_fn, logger)\n",
    "# training_loop.fit(train_dataloader, test_dataloader, 5)\n",
    "# save_model(resid_model, 'food100_conv_baseline', 'data/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d6e6f",
   "metadata": {},
   "source": [
    "## GAN\n",
    "\n",
    "Let's make anime faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6157c434",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/anime_faces/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m             image = \u001b[38;5;28mself\u001b[39m.transform(image)\n\u001b[32m     18\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m image\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m anime_dataset = \u001b[43mAnimeDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/anime_faces/images\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConvertImageDtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mResize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_items\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m anime_loader = DataLoader(anime_dataset, \u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mAnimeDataset.__init__\u001b[39m\u001b[34m(self, image_dir, transform, max_items)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_dir, transform=\u001b[38;5;28;01mNone\u001b[39;00m, max_items=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28mself\u001b[39m.img_labels = \u001b[38;5;28mlist\u001b[39m(\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mself\u001b[39m.img_dir = image_dir\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.transform = transform\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/anime_faces/images'"
     ]
    }
   ],
   "source": [
    "class AnimeDataset:\n",
    "    def __init__(self, image_dir, transform=None, max_items=None):\n",
    "        self.img_labels = list(os.listdir(image_dir))\n",
    "        self.img_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self._max_items = max_items\n",
    "\n",
    "    def __len__(self):\n",
    "        if self._max_items is None:\n",
    "            return len(self.img_labels)\n",
    "        return self._max_items\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        image = torchvision.io.decode_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "anime_dataset = AnimeDataset(\n",
    "    'data/anime_faces/images',\n",
    "    transform=Compose([\n",
    "        torchvision.transforms.ConvertImageDtype(torch.float32),\n",
    "        torchvision.transforms.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "        Resize((64, 64))]\n",
    "    ),\n",
    "    max_items=None)\n",
    "\n",
    "anime_loader = DataLoader(anime_dataset, 64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c100f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization https://docs.pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _model_path(path, index):\n",
    "    return f'{path}_{index}.pth'\n",
    "\n",
    "def save_tuned_model(loop: GANLoop, path: str, index: int, meta: dict | None = None):\n",
    "    directory, name = os.path.split(path)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    full_path = _model_path(path, index)\n",
    "    loop.save_checkpoint(full_path, meta)\n",
    "\n",
    "def load_tuned_model(path, index) -> tuple[GANLoop, dict]:\n",
    "    full_path = _model_path(path, index)\n",
    "    loop, meta = GANLoop.load_checkpoint(full_path)\n",
    "    return loop, meta\n",
    "\n",
    "def plot_loop_losses(g_loss, d_loss):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "    ax.plot(range(len(g_loss)), g_loss, label='generator loss')\n",
    "    ax.plot(range(len(d_loss)), d_loss, label='discriminator loss')\n",
    "    fig.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c153821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan_loop(learning_rate: float, n_generator_features: int, n_discriminator_features: int, generator_input_size: int = 100, dropout: float = 0.) -> GANLoop:\n",
    "    generator = Generator(generator_input_size, n_generator_features, dropout=dropout)\n",
    "    discriminator = Discriminator(n_discriminator_features, dropout=dropout)\n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "\n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "    loop = GANLoop(\n",
    "        generator,\n",
    "        discriminator,\n",
    "        torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(.5, .999)),\n",
    "        torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(.5, .999)),\n",
    "        loss\n",
    "    )\n",
    "    return loop\n",
    "\n",
    "\n",
    "class TuneGANModel:\n",
    "# TODO: TuneGANModel is close to a generic HP tuner.\n",
    "#       Not implemented is a validation set for learning on\n",
    "#       labeled data. Add that, then move this to nn_loop.\n",
    "    def __init__(self, build_loop: Callable[..., GANLoop], paramgrid: dict):\n",
    "        self.build = build_loop\n",
    "        self.paramgrid = paramgrid\n",
    "\n",
    "    def select_parameters(self, n_combinations: int) -> list[dict]:\n",
    "        \"\"\"Return n randomly selected hyperparameter combinations from the parameter grid.\"\"\"\n",
    "        parameters_array = np.array(list((itertools.product(*self.paramgrid.values()))))\n",
    "        parameters_indexes = np.random.choice(\n",
    "            np.arange(0, parameters_array.shape[0], step=1, dtype=np.int64),\n",
    "            min(n_combinations, parameters_array.shape[0]),\n",
    "            replace=False)\n",
    "        selected_parameters = parameters_array[parameters_indexes]\n",
    "        return [self._to_parameter_dict(params) for params in selected_parameters]\n",
    "\n",
    "    def _to_parameter_dict(self, parameters):\n",
    "        \"\"\"Map parameters to parameter names and conform type to self.build annotations.\"\"\"\n",
    "        build_parameters = inspect.signature(self.build).parameters\n",
    "        type_dict = {name: param.annotation for name, param in build_parameters.items() if type(param.annotation) is type}\n",
    "        out = {}\n",
    "        for param_name, param_value in zip(self.paramgrid, parameters):\n",
    "            param_type = type_dict.get(param_name, type(param_value))\n",
    "            out[param_name] = param_type(param_value)\n",
    "        return out\n",
    "\n",
    "    def tune_model(self, data: DataLoader, n_models: int, epochs: int, max_steps: int, callbacks: list | None):\n",
    "        selected_parameters = self.select_parameters(n_models)\n",
    "        for parameters in selected_parameters:\n",
    "            print(parameters)\n",
    "            loop = self.build(**parameters)\n",
    "\n",
    "            callbacks = callbacks if callbacks is not None else [\n",
    "                EndOnBatchCallback(max_steps),\n",
    "                VerboseTrainingCallback(max_steps // 4),\n",
    "                SnapshotCallback(8, interval=max_steps // 4)\n",
    "            ]\n",
    "\n",
    "            loop.fit(data, epochs, callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83250421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTuneGANMOdel:\n",
    "    def test_select_params(self):\n",
    "        model = TuneGANModel(build_gan_loop, {'dropout': [.01, .2], 'learning_rates': [.01, .0001], 'generator_input_size': [1, 2]})\n",
    "        selected = model.select_parameters(2)\n",
    "        assert len(selected) == 2\n",
    "        assert 'dropout' in selected[0] and 'learning_rates' in selected[0]\n",
    "        gen_size_type = type(selected[0]['generator_input_size'])\n",
    "        assert gen_size_type is int, f\"Expected int, but it is {gen_size_type}\"\n",
    "\n",
    "TestTuneGANMOdel().test_select_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc1e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = np.logspace(-3, -4.5, num=20)\n",
    "\n",
    "parameter_grid = {\n",
    "    'learning_rate': learning_rates.tolist(),\n",
    "    'n_generator_features': [36, 40, 44, 48, 60],\n",
    "    'n_discriminator_features': [36, 40, 44, 48, 60],\n",
    "    'generator_input_size': [100, 150]\n",
    "}\n",
    "\n",
    "gan_tuner = TuneGANModel(build_gan_loop, parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_tuner.tune_model(anime_loader, 3, 1, 10, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68550068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think all this is deprecated.\n",
    "\n",
    "# def tune_learning_rate(rates: list, n_features: int, n_combinations: int, max_steps: int, epochs: int = 1):\n",
    "\n",
    "#     results = {}\n",
    "#     max_steps = min(len(anime_dataset), max_steps)\n",
    "#     snapshot_interval = max_steps // 8 if max_steps is not None else None\n",
    "#     for idx in parameters:\n",
    "#         learning_rate, n_gen_features, n_dis_features = parameters[idx]\n",
    "#         dis_rate = learning_rate\n",
    "#         print(\n",
    "#             f\"Starting with {idx} generator rate {learning_rate}, discriminator rate {dis_rate}\" +\n",
    "#             f\"generator features: {n_gen_features} discriminator features: {n_dis_features}\"\n",
    "#         )\n",
    "#         random_inputs = torch.randn((8, generator.input_size, 1, 1)).to(device)\n",
    "#         input_one = random_inputs[0].view(1, generator.input_size, 1, 1)\n",
    "\n",
    "#         images = SnapshotCallback(8, 'epoch_end', 1)\n",
    "#         snapshots = SnapshotCallback(1, 'batch_end', max_steps // 8)\n",
    "#         callbacks = [VerboseTrainingCallback(max_steps // 4), snapshots, images]\n",
    "\n",
    "#         if max_steps:\n",
    "#             callbacks.append(EndOnBatch(max_steps))\n",
    "\n",
    "#         loop.fit(anime_loader, epochs, callbacks)\n",
    "#         selected_parameters = {'gen_rate': learning_rate, 'dis_rate': dis_rate, 'gen_features': n_gen_features, 'dis_features': n_dis_features}\n",
    "#         results[idx] = {'parameters': selected_parameters, 'images': images.snapshots, 'snapshots': snapshots.snapshots}\n",
    "#         save_tuned_model(loop, 'gan_tuning/model', idx, results[idx])\n",
    "#         show_image(make_grid(images.snapshots[-1].to('cpu'), nrow=4))\n",
    "#         plt.show()\n",
    "#         plot_loop_losses(loop.history['gen_loss'], loop.history['dis_loss'])\n",
    "#         plt.show()\n",
    "\n",
    "#         print()\n",
    "#     # capture.save('gan_tuning/generator_images.pth')\n",
    "#     return results\n",
    "\n",
    "\n",
    "# results = tune_learning_rate(learning_rates.tolist(), 10, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34547635",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, r in results.items():\n",
    "    print(k)\n",
    "    show_image(make_grid(r['images'][0].to('cpu'), nrow=4))\n",
    "    show_image(make_grid(torch.concat(r['snapshots']).to('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{int(k): (float(v['gen_rate']), v['dis_rate']) for k, v in results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_loop = 112\n",
    "try:\n",
    "    loop, meta = GANLoop.load_checkpoint((f'well_tuned_{selected_loop}.pth'))\n",
    "except FileNotFoundError:\n",
    "    print(\"Not found.\")\n",
    "    loop, meta = load_tuned_model('gan_tuning/model', selected_loop)\n",
    "random_inputs = torch.randn((8, loop.generator.input_size, 1, 1)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    show_image(make_grid(loop.generator(random_inputs).cpu()))\n",
    "    step = 0\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for gen_loss, dis_loss in loop.step(anime_loader):\n",
    "        if step % (len(anime_dataset) // anime_loader.batch_size // 5) == 0:\n",
    "            print(f\"Gen loss: {gen_loss}, dis loss: {dis_loss}\")\n",
    "        step += 1\n",
    "\n",
    "show_image(make_grid(loop.generator(random_inputs).cpu()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop.save_checkpoint(f'well_tuned_{selected_loop}.pth', meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(loop.generator(random_inputs.to(device)).cpu()[-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_result(result: dict):\n",
    "    x = np.arange(0, len(result['gen_loss']))\n",
    "    plt.plot(x, result['gen_loss'], label='generator loss')\n",
    "    plt.plot(x, result['dis_loss'], label='dis_loss')\n",
    "\n",
    "report_result(meta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
