{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768dc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToTensor, Compose, Resize\n",
    "\n",
    "from philosofool.torch.nn_models import (\n",
    "    ResidualBlock, ResidualNetwork, NeuralNetwork,\n",
    "    Generator, Discriminator,\n",
    "    compute_convolution_dims, conv_dims_1d\n",
    ")\n",
    "from philosofool.torch.nn_loop import TrainingLoop, JSONLogger, CompositeLogger, StandardOutputLogger, GANLoop, TrainingLoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img):\n",
    "    \"\"\"Show image implied by input tensor. The input is assumed to be normalized.\"\"\"\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.detach().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92195e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(path) -> str:\n",
    "    with open(path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def get_fashionMINST_data() -> tuple[DataLoader, DataLoader, dict]:\n",
    "\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root='data', train=True, download=True, transform=ToTensor()\n",
    "    )\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root='data', train=False, download=True, transform=ToTensor()\n",
    "    )\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    classes = {\n",
    "        0: 'T-shirt/top',\n",
    "        1: 'Trouser',\n",
    "        2: 'Pullover',\n",
    "        3: 'Dress',\n",
    "        4: 'Coat',\n",
    "        5: 'Sandal',\n",
    "        6: 'Shirt',\n",
    "        7: 'Sneaker',\n",
    "        8: 'Bag',\n",
    "        9: 'Ankle'}\n",
    "    return train_dataloader, test_dataloader, classes\n",
    "\n",
    "def get_food100_data():\n",
    "    transform = Compose([Resize((256, 256)), ToTensor()])\n",
    "    training_data = datasets.Food101(\n",
    "        root='data', split='train', download=True, transform=transform\n",
    "    )\n",
    "    test_data = datasets.Food101(\n",
    "        root='data', split='test', download=True, transform=transform\n",
    "    )\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(\n",
    "        training_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "    classes_path = os.path.join(os.getcwd(), 'data/food-101/meta/classes.txt')\n",
    "    classes = dict(enumerate(read_text(classes_path).split('\\n')))\n",
    "\n",
    "    return train_dataloader, test_dataloader, classes\n",
    "\n",
    "\n",
    "train_dataloader, test_dataloader, classes = get_fashionMINST_data()\n",
    "# train_dataloader, test_dataloader, classes = get_food100_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c700a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "input_dims = tuple(int(x) for x in X.shape[2:])\n",
    "in_channels = int(X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc1549",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "configurations = [\n",
    "    ('baseline', {'n_hidden_layers': 1, 'width': 256}),\n",
    "    ('h2_w128', {'n_hidden_layers': 2, 'width': 128}),\n",
    "    ('h3_w94', {'n_hidden_layers': 3, 'width': 96}),\n",
    "    ('h4_w64', {'n_hidden_layers': 4, 'width': 64})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593026cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_experiment(configurations):\n",
    "    X, _ = next(iter(test_dataloader))\n",
    "    input_dims = int(X.shape[2] * X.shape[3])\n",
    "    input_channels = int(X.shape[1])\n",
    "    for model_name, config in configurations:\n",
    "        model = NeuralNetwork(input_dims * input_channels, len(classes), **config)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        logger = CompositeLogger(JSONLogger('data/logs/food100_' + model_name + '.json'), StandardOutputLogger(500))\n",
    "        training_loop = TrainingLoop(model, optimizer, loss_fn, logger)\n",
    "        print(model_name)\n",
    "        training_loop.fit(train_dataloader, test_dataloader, epochs=1)\n",
    "\n",
    "# model_experiment(configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a97429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_model_path(model: nn.Module, model_name: str, directory: str | None) -> str:\n",
    "    string_hash = hashlib.sha1(bytes(str(model).encode())).hexdigest()\n",
    "    full_path = model_name + '_' + string_hash+ '.model'\n",
    "    if directory:\n",
    "        full_path = os.path.join(directory, full_path)\n",
    "    return os.path.normpath(full_path)\n",
    "\n",
    "\n",
    "def save_model(model: nn.Module, model_name: str, directory: str | None):\n",
    "    \"\"\"Save model, using string repr to assure that models are not duplicated once trained.\"\"\"\n",
    "    full_path = _make_model_path(model, model_name, directory)\n",
    "    if not os.path.exists(os.path.dirname(full_path)):\n",
    "        os.makedirs(os.path.dirname(full_path))\n",
    "    torch.save(model, full_path)\n",
    "\n",
    "def load_model(model: nn.Module, model_name: str, directory: str | None):\n",
    "    full_path = _make_model_path(model, model_name, directory)\n",
    "    torch.load(full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_model = ResidualNetwork((256, 256), in_channels, 101).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6404de",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(resid_model.parameters(), lr=1e-3)\n",
    "logger = CompositeLogger(JSONLogger('food100_conv_baseline.json'), StandardOutputLogger(100))\n",
    "\n",
    "training_loop = TrainingLoop(resid_model, optimizer, loss_fn, logger)\n",
    "# training_loop.fit(train_dataloader, test_dataloader, 5)\n",
    "# save_model(resid_model, 'food100_conv_baseline', 'data/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d6e6f",
   "metadata": {},
   "source": [
    "## GAN\n",
    "\n",
    "Let's make anime faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeDataset:\n",
    "    def __init__(self, image_dir, transform=None, max_items=None):\n",
    "        self.img_labels = list(os.listdir(image_dir))\n",
    "        self.img_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self._max_items = max_items\n",
    "\n",
    "    def __len__(self):\n",
    "        if self._max_items is None:\n",
    "            return len(self.img_labels)\n",
    "        return self._max_items\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        image = torchvision.io.decode_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "anime_dataset = AnimeDataset(\n",
    "    'data/anime_faces/images',\n",
    "    transform=Compose([\n",
    "        torchvision.transforms.ConvertImageDtype(torch.float32),\n",
    "        torchvision.transforms.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "        Resize((64, 64))]\n",
    "    ),\n",
    "    max_items=None)\n",
    "\n",
    "anime_loader = DataLoader(anime_dataset, 64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c100f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization https://docs.pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6eecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adversaries(n_features: int = 16):\n",
    "    generator = Generator(100, n_features, dropout=0.)\n",
    "    discriminator = Discriminator(n_features, dropout=0.)\n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptureGeneratedImages:\n",
    "    def __init__(self, n_images: int):\n",
    "        self.n_images = n_images\n",
    "        self._captured = []\n",
    "\n",
    "    def capture(self, generator: Generator):\n",
    "        random_inputs = torch.randn((self.n_images, generator.input_size, 1, 1)).to(device)\n",
    "        tensors = generator(random_inputs)\n",
    "        self._captured.append(tensors)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self._captured, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndOnBatch:\n",
    "    \"\"\"After a number of batches, proceed to the next epoch.\"\"\"\n",
    "    def __init__(self, end_on: int):\n",
    "        self.end_on = end_on\n",
    "        self.on = 'batch_end'\n",
    "\n",
    "    def __call__(self, loop, batch: int, **kwargs):\n",
    "        if batch == self.end_on:\n",
    "            return 'end_batch'\n",
    "\n",
    "def test_end_on_batch():\n",
    "    end_on_batch = EndOnBatch(1)\n",
    "    assert end_on_batch.on == 'batch_end'\n",
    "    assert end_on_batch(None, batch=1, gen_loss=.1) == 'end_batch'\n",
    "    assert end_on_batch(None, batch=2) is None\n",
    "\n",
    "\n",
    "class SnapshotCallback:\n",
    "    \"\"\"Collect snapshots on an interval.\"\"\"\n",
    "    def __init__(self, n_images: int, on: str, interval: int):\n",
    "        self.interval = interval\n",
    "        self.n_images = n_images\n",
    "        self.on = on\n",
    "        self.snapshots = []\n",
    "        self._random_inputs = {}\n",
    "\n",
    "    def __call__(self, loop, **kwargs):\n",
    "        interval = self._get_interval(kwargs)\n",
    "        if interval % self.interval != 0:\n",
    "            return\n",
    "        random_input = self._random_input(loop.generator.input_size).to(loop._device)\n",
    "        result = loop.generator(random_input)\n",
    "        self.snapshots.append(result)\n",
    "\n",
    "    def _get_interval(self, kwargs):\n",
    "        if self.on == 'batch_end':\n",
    "            return kwargs['batch']\n",
    "        if self.on == 'epoch_end':\n",
    "            return kwargs['epoch']\n",
    "        # for example, to call on 'fit_end': assume interval is one.\n",
    "        return 1\n",
    "\n",
    "    def _random_input(self, size: int):\n",
    "        if size in self._random_inputs:\n",
    "            return self._random_inputs[size]\n",
    "        random_input = torch.randn((self.n_images, size, 1, 1))\n",
    "        self._random_inputs[size] = random_input\n",
    "        return random_input\n",
    "\n",
    "\n",
    "def test_snapshot_callback():\n",
    "    generator, discriminator = make_adversaries(6)\n",
    "\n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "    loop = GANLoop(\n",
    "        generator,\n",
    "        discriminator,\n",
    "        torch.optim.SGD(generator.parameters(), lr=.01, momentum=0),\n",
    "        torch.optim.SGD(discriminator.parameters(), lr=.01, momentum=0),\n",
    "        loss\n",
    "    )\n",
    "    loop.generator.eval()\n",
    "    snapshot_callback = SnapshotCallback(n_images=1, interval=2, on='batch_end')\n",
    "    assert snapshot_callback.on == 'batch_end'\n",
    "    snapshot_callback(loop, batch=2, gen_loss=.1, dis_loss=.1)\n",
    "    assert len(snapshot_callback.snapshots ) == 1, \"Snapshots should update when called on batch interval.\"\n",
    "    snapshot_callback(loop, batch=3, gen_loss=.1, dis_loss=.1)\n",
    "    assert len(snapshot_callback.snapshots ) == 1, \"Snapshots should update only on interval.\"\n",
    "    snapshot_callback(loop, batch=2, gen_loss=.1, dis_loss=.1)\n",
    "    assert len(snapshot_callback.snapshots) == 2, \"Snapshots should update when called on batch interval.\"\n",
    "    snapshots = snapshot_callback.snapshots\n",
    "    assert torch.allclose(snapshots[0].detach(), snapshots[1].detach())\n",
    "\n",
    "    snapshot_callback = SnapshotCallback(n_images=8, interval=1, on='epoch_end')\n",
    "    snapshot_callback(loop, epoch=1)\n",
    "    assert snapshot_callback.snapshots[0].shape[0] == snapshot_callback.n_images == 8\n",
    "\n",
    "class StandardOutputCallback:\n",
    "    def __init__(self, interval: int):\n",
    "        self.on = 'batch_end'\n",
    "        self.interval = interval\n",
    "\n",
    "    def __call__(self, loop, **kwargs):\n",
    "        batch = kwargs['batch']\n",
    "        if batch % self.interval != 0:\n",
    "            return\n",
    "        strings = []\n",
    "        for key, value in kwargs.items():\n",
    "            strings.append(f\"{key}: {value}\")\n",
    "        print(', '.join(strings))\n",
    "\n",
    "def test_standard_ouput_callback():\n",
    "\n",
    "    callback = StandardOutputCallback(interval=2)\n",
    "    assert callback.on == 'batch_end'\n",
    "    callback(None, batch=1)\n",
    "    callback(None, batch=2, gen_loss=.1, dis_loss=.2)\n",
    "\n",
    "test_end_on_batch()\n",
    "test_snapshot_callback()\n",
    "test_standard_ouput_callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _model_path(path, index):\n",
    "    return f'{path}_{index}.pth'\n",
    "\n",
    "def save_tuned_model(loop: GANLoop, path: str, index: int, meta: dict | None = None):\n",
    "    directory, name = os.path.split(path)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    full_path = _model_path(path, index)\n",
    "    loop.save_checkpoint(full_path, meta)\n",
    "\n",
    "def load_tuned_model(path, index) -> tuple[GANLoop, dict]:\n",
    "    full_path = _model_path(path, index)\n",
    "    loop, meta = GANLoop.load_checkpoint(full_path)\n",
    "    return loop, meta\n",
    "\n",
    "def plot_loop_losses(g_loss, d_loss):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "    ax.plot(range(len(g_loss)), g_loss, label='generator loss')\n",
    "    ax.plot(range(len(d_loss)), d_loss, label='discriminator loss')\n",
    "    fig.legend()\n",
    "\n",
    "def tune_learning_rate(rates: list, n_combinations: int, max_steps: int, epochs: int = 1):\n",
    "    import itertools\n",
    "    rate_pairs = np.array(list((itertools.product(rates, rates))))\n",
    "\n",
    "    rate_indexes = np.random.choice(\n",
    "        np.arange(0, rate_pairs.shape[0], step=1, dtype=np.int64),\n",
    "        min(n_combinations, rate_pairs.shape[0]),\n",
    "        replace=False)\n",
    "\n",
    "    results = {}\n",
    "    max_steps = min(len(anime_dataset), max_steps)\n",
    "    snapshot_interval = max_steps // 8 if max_steps is not None else None\n",
    "    capture = CaptureGeneratedImages(8)\n",
    "    for idx in rate_indexes:\n",
    "        gen_rate, dis_rate = rate_pairs[idx]\n",
    "        print(f\"Starting with {idx} generator rate {gen_rate}, discriminator rate {dis_rate}\")\n",
    "        generator, discriminator = make_adversaries(32)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()\n",
    "        loop = GANLoop(\n",
    "            generator,\n",
    "            discriminator,\n",
    "            torch.optim.Adam(generator.parameters(), lr=gen_rate, betas=(.5, .999)),\n",
    "            # torch.optim.SGD(generator.parameters(), lr=gen_rate, momentum=0),\n",
    "            torch.optim.Adam(discriminator.parameters(), lr=dis_rate, betas=(.5, .999)),\n",
    "            # torch.optim.SGD(discriminator.parameters(), lr=dis_rate, momentum=.1),\n",
    "            loss\n",
    "        )\n",
    "        random_inputs = torch.randn((8, generator.input_size, 1, 1)).to(device)\n",
    "        input_one = random_inputs[0].view(1, generator.input_size, 1, 1)\n",
    "\n",
    "        images = SnapshotCallback(8, 'epoch_end', 1)\n",
    "        snapshots = SnapshotCallback(1, 'batch_end', max_steps // 8)\n",
    "        callbacks = [StandardOutputCallback(max_steps // 4), snapshots, images]\n",
    "\n",
    "        if max_steps:\n",
    "            callbacks.append(EndOnBatch(max_steps))\n",
    "\n",
    "        loop.fit(anime_loader, epochs, callbacks)\n",
    "        results[idx] = {'gen_rate': gen_rate, 'dis_rate': dis_rate, 'images': images.snapshots, 'snapshots': snapshots.snapshots}\n",
    "        save_tuned_model(loop, 'gan_tuning/model', idx, results[idx])\n",
    "        show_image(make_grid(images.snapshots[-1].to('cpu'), nrow=4))\n",
    "        plt.show()\n",
    "        plot_loop_losses(loop.history['gen_loss'], loop.history['dis_loss'])\n",
    "        plt.show()\n",
    "\n",
    "        print()\n",
    "    # capture.save('gan_tuning/generator_images.pth')\n",
    "    return results\n",
    "\n",
    "\n",
    "learning_rates = np.logspace(-3, -4.5, num=20)\n",
    "results = tune_learning_rate(learning_rates.tolist(), 10, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34547635",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, r in results.items():\n",
    "    print(k)\n",
    "    show_image(make_grid(r['images'][0].to('cpu'), nrow=4))\n",
    "    show_image(make_grid(torch.concat(r['snapshots']).to('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{int(k): (float(v['gen_rate']), v['dis_rate']) for k, v in results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_loop = 112\n",
    "try:\n",
    "    loop, meta = GANLoop.load_checkpoint((f'well_tuned_{selected_loop}.pth'))\n",
    "except FileNotFoundError:\n",
    "    print(\"Not found.\")\n",
    "    loop, meta = load_tuned_model('gan_tuning/model', selected_loop)\n",
    "random_inputs = torch.randn((8, loop.generator.input_size, 1, 1)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    show_image(make_grid(loop.generator(random_inputs).cpu()))\n",
    "    step = 0\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for gen_loss, dis_loss in loop.step(anime_loader):\n",
    "        if step % (len(anime_dataset) // anime_loader.batch_size // 5) == 0:\n",
    "            print(f\"Gen loss: {gen_loss}, dis loss: {dis_loss}\")\n",
    "        step += 1\n",
    "\n",
    "show_image(make_grid(loop.generator(random_inputs).cpu()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop.save_checkpoint(f'well_tuned_{selected_loop}.pth', meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(loop.generator(random_inputs.to(device)).cpu()[-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_result(result: dict):\n",
    "    x = np.arange(0, len(result['gen_loss']))\n",
    "    plt.plot(x, result['gen_loss'], label='generator loss')\n",
    "    plt.plot(x, result['dis_loss'], label='dis_loss')\n",
    "\n",
    "report_result(meta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
