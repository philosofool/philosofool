{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768dc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "import hashlib\n",
    "import inspect\n",
    "import itertools\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToTensor, Compose, Resize\n",
    "\n",
    "from philosofool.torch.nn_models import (\n",
    "    ResidualBlock, ResidualNetwork, NeuralNetwork,\n",
    "    Generator, Discriminator,\n",
    "    compute_convolution_dims, conv_dims_1d\n",
    ")\n",
    "from philosofool.torch.nn_loop import (\n",
    "    TrainingLoop, JSONLogger, CompositeLogger, StandardOutputLogger, GANLoop, TrainingLoop,\n",
    "    EndOnBatchCallback, SnapshotCallback, VerboseTrainingCallback\n",
    ")\n",
    "from philosofool.torch.visualize import show_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92195e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(path) -> str:\n",
    "    with open(path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def get_fashionMINST_data() -> tuple[DataLoader, DataLoader, dict]:\n",
    "\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root='data', train=True, download=True, transform=ToTensor()\n",
    "    )\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root='data', train=False, download=True, transform=ToTensor()\n",
    "    )\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    classes = {\n",
    "        0: 'T-shirt/top',\n",
    "        1: 'Trouser',\n",
    "        2: 'Pullover',\n",
    "        3: 'Dress',\n",
    "        4: 'Coat',\n",
    "        5: 'Sandal',\n",
    "        6: 'Shirt',\n",
    "        7: 'Sneaker',\n",
    "        8: 'Bag',\n",
    "        9: 'Ankle'}\n",
    "    return train_dataloader, test_dataloader, classes\n",
    "\n",
    "def get_food100_data():\n",
    "    transform = Compose([Resize((256, 256)), ToTensor()])\n",
    "    training_data = datasets.Food101(\n",
    "        root='data', split='train', download=True, transform=transform\n",
    "    )\n",
    "    test_data = datasets.Food101(\n",
    "        root='data', split='test', download=True, transform=transform\n",
    "    )\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(\n",
    "        training_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "    classes_path = os.path.join(os.getcwd(), 'data/food-101/meta/classes.txt')\n",
    "    classes = dict(enumerate(read_text(classes_path).split('\\n')))\n",
    "\n",
    "    return train_dataloader, test_dataloader, classes\n",
    "\n",
    "\n",
    "train_dataloader, test_dataloader, classes = get_fashionMINST_data()\n",
    "# train_dataloader, test_dataloader, classes = get_food100_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c700a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "input_dims = tuple(int(x) for x in X.shape[2:])\n",
    "in_channels = int(X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc1549",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "configurations = [\n",
    "    ('baseline', {'n_hidden_layers': 1, 'width': 256}),\n",
    "    ('h2_w128', {'n_hidden_layers': 2, 'width': 128}),\n",
    "    ('h3_w94', {'n_hidden_layers': 3, 'width': 96}),\n",
    "    ('h4_w64', {'n_hidden_layers': 4, 'width': 64})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593026cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_experiment(configurations):\n",
    "    X, _ = next(iter(test_dataloader))\n",
    "    input_dims = int(X.shape[2] * X.shape[3])\n",
    "    input_channels = int(X.shape[1])\n",
    "    for model_name, config in configurations:\n",
    "        model = NeuralNetwork(input_dims * input_channels, len(classes), **config)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        logger = CompositeLogger(JSONLogger('data/logs/food100_' + model_name + '.json'), StandardOutputLogger(500))\n",
    "        training_loop = TrainingLoop(model, optimizer, loss_fn, logger)\n",
    "        print(model_name)\n",
    "        training_loop.fit(train_dataloader, test_dataloader, epochs=1)\n",
    "\n",
    "# model_experiment(configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a97429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_model_path(model: nn.Module, model_name: str, directory: str | None) -> str:\n",
    "    string_hash = hashlib.sha1(bytes(str(model).encode())).hexdigest()\n",
    "    full_path = model_name + '_' + string_hash+ '.model'\n",
    "    if directory:\n",
    "        full_path = os.path.join(directory, full_path)\n",
    "    return os.path.normpath(full_path)\n",
    "\n",
    "\n",
    "def save_model(model: nn.Module, model_name: str, directory: str | None):\n",
    "    \"\"\"Save model, using string repr to assure that models are not duplicated once trained.\"\"\"\n",
    "    full_path = _make_model_path(model, model_name, directory)\n",
    "    if not os.path.exists(os.path.dirname(full_path)):\n",
    "        os.makedirs(os.path.dirname(full_path))\n",
    "    torch.save(model, full_path)\n",
    "\n",
    "def load_model(model: nn.Module, model_name: str, directory: str | None):\n",
    "    full_path = _make_model_path(model, model_name, directory)\n",
    "    torch.load(full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_model = ResidualNetwork((256, 256), in_channels, 101).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6404de",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(resid_model.parameters(), lr=1e-3)\n",
    "logger = CompositeLogger(JSONLogger('food100_conv_baseline.json'), StandardOutputLogger(100))\n",
    "\n",
    "training_loop = TrainingLoop(resid_model, optimizer, loss_fn, logger)\n",
    "# training_loop.fit(train_dataloader, test_dataloader, 5)\n",
    "# save_model(resid_model, 'food100_conv_baseline', 'data/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d6e6f",
   "metadata": {},
   "source": [
    "## GAN\n",
    "\n",
    "Let's make anime faces.\n",
    "\n",
    "This was a project in working on GAN models. \n",
    "\"GAN\" stands for geneterative adversarial network, and it's a process for training models\n",
    "that can simulate the distributions of known examples.\n",
    "A GAN works by having a generator and discriminator learn simultaneously.\n",
    "If you're not familiar, search it on the internet. There are a lot of good explanations.\n",
    "\n",
    "### Issues\n",
    "\n",
    "Let me mention a bit of what I learned while working with GANs.\n",
    "First, issues I ran into:\n",
    "1. Mode collapse: the generator produces the same image for all inputs.\n",
    "Obviously, the goal is for the generator to produce diverse outputs\n",
    "from diverse inputs.\n",
    "Mode collapse is a situation in which the generator learns a single output\n",
    "and won't leave that spot. \n",
    "2. Stalling. (Not sure if there's a technical term for this) The generator\n",
    "reaches a state of clear improvement and then stops making progress, or regresses.\n",
    "3. Lack of metric measurements. \n",
    "This is simiple: there's not an easy number to associate with progress.\n",
    "In short, it's not possible to tell when the model has converged.\n",
    "\n",
    "### Lessons\n",
    "\n",
    "The discriminator and generator need to be suited adversaries, so to speak. \n",
    "When one has significantly greater capacity than the other, it lead to mode collapse.\n",
    "Additionally, both need a lot of capacity for this problem, generating 64x64 anime faces.\n",
    "I tried some things that didn't work very well. \n",
    "I thought maybe just getting the models learning rates right, relative to one another,\n",
    "might solve the problem. \n",
    "It did, sort of, but without enough capacity, there was stalling.\n",
    "By the end, I cranked up capacity a lot in order to produce somewhat convincing images.\n",
    "\n",
    "Stalling was pretty persistent. It's difficult, without metrics, to say when you have stalled.\n",
    "\n",
    "This all lead to a lot of babysitting. That's the pejorative word for having to watch over models manually.\n",
    "The lack of a metric was a major cause. Without a metric, it wasn't possible to implement early stopping,\n",
    "programmatically select tuned hyperparameters which lead to progress, or assess the effects of hyperparameters.\n",
    "\n",
    "I ended up literally looking at the generated outputs and selecting based on apparent diversity,\n",
    "then training for several loops. Usually this stalled and I had to start over. \n",
    "I plan to add capacity and come back to this to see if I can get a very good model\n",
    "in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeDataset:\n",
    "    def __init__(self, image_dir, transform=None, max_items=None):\n",
    "        self.img_labels = list(os.listdir(image_dir))\n",
    "        self.img_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self._max_items = max_items\n",
    "\n",
    "    def __len__(self):\n",
    "        if self._max_items is None:\n",
    "            return len(self.img_labels)\n",
    "        return self._max_items\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        image = torchvision.io.decode_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "anime_dataset = AnimeDataset(\n",
    "    'data/anime_faces/images',\n",
    "    transform=Compose([\n",
    "        torchvision.transforms.ConvertImageDtype(torch.float32),\n",
    "        torchvision.transforms.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "        Resize((64, 64))]\n",
    "    ),\n",
    "    max_items=None)\n",
    "\n",
    "anime_loader = DataLoader(anime_dataset, 32, shuffle=True, num_workers=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def optimize_loader(dataset):\n",
    "    \"\"\"Find fast data itertor to save time training.\"\"\"\n",
    "    best_time = np.inf\n",
    "    for batch_size in [16, 32]:\n",
    "        for n_workers in range(3, 12):\n",
    "            model = Discriminator(30).to(device)\n",
    "            data_loader = DataLoader(dataset, batch_size, shuffle=True, num_workers=n_workers)\n",
    "            start = time()\n",
    "            for i, e in enumerate(data_loader):\n",
    "                e = e.to(device)\n",
    "\n",
    "                if i * batch_size >= 1280:\n",
    "                    break\n",
    "                model(e)\n",
    "            end = time()\n",
    "            current = end - start\n",
    "            if current < best_time:\n",
    "                best = (batch_size, n_workers)\n",
    "                best_time = current\n",
    "            print(f\"for {batch_size} with {n_workers} took {end - start}\")\n",
    "    print(f\"Best time was {best} with {best_time}\")\n",
    "\n",
    "# optimize_loader(dataset=anime_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c100f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization https://docs.pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c153821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan_loop(learning_rate: float, n_generator_features: int, n_discriminator_features: int, generator_input_size: int = 100, dropout: float = 0.) -> GANLoop:\n",
    "    \"\"\"Take a specification of hyperparameters and return a configured GANModel.\"\"\"\n",
    "    generator = Generator(generator_input_size, n_generator_features, dropout=dropout)\n",
    "    discriminator = Discriminator(n_discriminator_features, dropout=dropout)\n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "\n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "    loop = GANLoop(\n",
    "        generator,\n",
    "        discriminator,\n",
    "        torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(.5, .999)),\n",
    "        torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(.5, .999)),\n",
    "        loss\n",
    "    )\n",
    "    return loop\n",
    "\n",
    "\n",
    "class TuneGANModel:\n",
    "# TODO: TuneGANModel is close to a generic HP tuner.\n",
    "#       Not implemented is a validation set for learning on\n",
    "#       labeled data. Add that, then move this to nn_loop.\n",
    "    def __init__(self, build_loop: Callable[..., GANLoop], paramgrid: dict):\n",
    "        self.build = build_loop\n",
    "        self.paramgrid = paramgrid\n",
    "\n",
    "    def select_parameters(self, n_combinations: int) -> list[dict]:\n",
    "        \"\"\"Return n randomly selected hyperparameter combinations from the parameter grid.\"\"\"\n",
    "        parameters_array = np.array(list((itertools.product(*self.paramgrid.values()))))\n",
    "        parameters_indexes = np.random.choice(\n",
    "            np.arange(0, parameters_array.shape[0], step=1, dtype=np.int64),\n",
    "            min(n_combinations, parameters_array.shape[0]),\n",
    "            replace=False)\n",
    "        selected_parameters = parameters_array[parameters_indexes]\n",
    "        return [self._to_parameter_dict(params) for params in selected_parameters]\n",
    "\n",
    "    def _to_parameter_dict(self, parameters):\n",
    "        \"\"\"Map parameters to parameter names and conform type to self.build annotations.\"\"\"\n",
    "        build_parameters = inspect.signature(self.build).parameters\n",
    "        type_dict = {name: param.annotation for name, param in build_parameters.items() if type(param.annotation) is type}\n",
    "        out = {}\n",
    "        for param_name, param_value in zip(self.paramgrid, parameters):\n",
    "            param_type = type_dict.get(param_name, type(param_value))\n",
    "            out[param_name] = param_type(param_value)\n",
    "        return out\n",
    "\n",
    "    def tune_model(self, data: DataLoader, n_models: int, epochs: int, max_steps: int, callbacks: list | None):\n",
    "        selected_parameters = self.select_parameters(n_models)\n",
    "        for parameters in selected_parameters:\n",
    "            print(parameters)\n",
    "            loop = self.build(**parameters)\n",
    "\n",
    "            callbacks = callbacks if callbacks is not None else []\n",
    "\n",
    "            loop.fit(data, epochs, callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83250421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTuneGANMOdel:\n",
    "    def test_select_params(self):\n",
    "        model = TuneGANModel(build_gan_loop, {'dropout': [.01, .2], 'learning_rates': [.01, .0001], 'generator_input_size': [1, 2]})\n",
    "        selected = model.select_parameters(2)\n",
    "        assert len(selected) == 2\n",
    "        assert 'dropout' in selected[0] and 'learning_rates' in selected[0]\n",
    "        gen_size_type = type(selected[0]['generator_input_size'])\n",
    "        assert gen_size_type is int, f\"Expected int, but it is {gen_size_type}\"\n",
    "\n",
    "TestTuneGANMOdel().test_select_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc1e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = np.logspace(-3, -4.5, num=20)\n",
    "\n",
    "parameter_grid = {\n",
    "    'learning_rate': learning_rates.tolist(),\n",
    "    'n_generator_features': [24, 36, 40, 44, 52, 60],\n",
    "    'n_discriminator_features': [24, 36, 40, 44, 52, 60],\n",
    "    'generator_input_size': [150, 300]\n",
    "}\n",
    "\n",
    "gan_tuner = TuneGANModel(build_gan_loop, parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100\n",
    "features_size = 16\n",
    "nn.Sequential(\n",
    "    nn.ConvTranspose2d(input_size, features_size * 8, 4, 2, 1),\n",
    "    nn.ConvTranspose2d(features_size * 8, features_size * 4, 8, 4, 0),\n",
    "    nn.ConvTranspose2d(features_size * 4, features_size * 2, 8, 2, 0),\n",
    "    nn.ConvTranspose2d(features_size * 2, features_size * 2, 8, 2, 1),\n",
    "    nn.Conv2d(features_size * 2, 3, 1, 1, 0)\n",
    ")(torch.randn(1, input_size, 1, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, we train several models and select hyperparameters which were successful\n",
    "# for further iterations.\n",
    "\n",
    "end_batch_early = False\n",
    "if end_batch_early:\n",
    "    max_steps = int(2**13 / anime_loader.batch_size)\n",
    "else:\n",
    "    max_steps = len(anime_loader.dataset) // anime_loader.batch_size\n",
    "callbacks = [\n",
    "    VerboseTrainingCallback(max_steps // 4),\n",
    "    SnapshotCallback(8, interval=max_steps // 4)\n",
    "]\n",
    "if end_batch_early:\n",
    "    callbacks.append(EndOnBatchCallback(max_steps))\n",
    "\n",
    "gan_tuner.tune_model(anime_loader, 10, 1, max_steps, callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use good parameters from above\n",
    "# NOTE: if we had a metric, this could be programmatic...\n",
    "\n",
    "params = {'learning_rate': 0.00013538761800225446, 'n_generator_features': 52, 'n_discriminator_features': 44, 'generator_input_size': 150}\n",
    "loop = build_gan_loop(**params)\n",
    "n_batches = len(anime_dataset) // anime_loader.batch_size\n",
    "callbacks = [\n",
    "    VerboseTrainingCallback(n_batches // 4),\n",
    "    SnapshotCallback(8, interval=n_batches // 4)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop.fit(anime_loader, 10, callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
